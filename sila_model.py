# -*- coding: utf-8 -*-
"""SiLa_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q9hUSUq8CT1y6cUitzvSNT5iXbR3q6Oy

# SiLa Model

### Import Library

Jika tidak bisa mengimport library harus menginstalnya terlebih dahulu

!pip install gdown
"""

import pandas as pd
import numpy as np
import os
import zipfile
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import tensorflow as tf
import json
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
import gdown

"""### Load data dari Google Drive"""

# Download file dari Google Drive
FILE_ID = "1MxfUU3zMAi6FmMfYoRWEDjAaqQM-bJqv"
url = f"https://drive.google.com/uc?id={FILE_ID}"
output = "dataset.zip"

gdown.download(url, output, quiet=False)

# Ekstrak isi file ZIP ke folder 'dataset'
with zipfile.ZipFile(output, "r") as zip_ref:
    zip_ref.extractall("dataset")

# Cek isi folder dataset
print("File CSV di folder dataset:", os.listdir("dataset"))

"""### Merge Data"""

folder_path = "/content/dataset/Dataset A-Z & Space"

def combine_all_csv(folder=folder_path):
    all_data = []
    # Urutkan nama file secara alfabetis (A.csv, B.csv, ..., Space.csv)
    for filename in sorted(os.listdir(folder)):
        if filename.endswith(".csv"):
            df = pd.read_csv(os.path.join(folder, filename))
            all_data.append(df)
    data = pd.concat(all_data, ignore_index=True)
    return data

if __name__ == "__main__":
    data = combine_all_csv()
    print(data.head())
    print(f"Jumlah data total: {len(data)}")
    data.to_csv("dataset_all.csv", index=False)
    print("[INFO] Dataset gabungan disimpan sebagai dataset_all.csv")

"""### Data Understanding"""

df_all = pd.read_csv("dataset_all.csv")
df_all.head()

df_all.info()

"""### Mengatasi Missing Values"""

df_all.isnull().sum()

"""Tidak ada missing value

### Mengatasi data duplikat
"""

print(f'Jumlah data duplikat:', df_all.duplicated().sum())

"""Tidak ada data duplikat

### Mengecek besaran data perlabel
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'data' dataframe is already loaded and contains a 'label' column
if 'label' in data.columns:
    # Count the occurrences of each label
    label_counts = data['label'].value_counts().sort_index()

    # Create the visualization
    plt.figure(figsize=(15, 6))
    sns.barplot(x=label_counts.index, y=label_counts.values, palette='viridis')
    plt.title('Jumlah Data per Label (A-Z)')
    plt.xlabel('Label')
    plt.ylabel('Jumlah Data')
    plt.xticks(rotation=0) # Keep labels horizontal for clarity
    plt.grid(axis='y', linestyle='--')
    plt.show()
else:
    print("Kolom 'label' tidak ditemukan dalam dataframe.")

"""Pada visualisasi terlihat bahwa besaran data setiap label seimbang sebesar 200 data perlabel

### Split data dan menyimpan label
"""

# Load dataset
data = pd.read_csv("dataset_all.csv")
X = data.drop("label", axis=1).values
y = data["label"].values

# Label Encoding
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Simpan label encoder ke file JSON
label_mapping = {label: int(idx) for idx, label in enumerate(le.classes_)}
with open("label.json", "w") as f:
    json.dump(label_mapping, f)
print("[INFO] label.json disimpan.")

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

input_dim = X.shape[1]
num_classes = len(le.classes_)

"""### Modeling"""

model = tf.keras.Sequential([
    tf.keras.layers.Dense(512, activation='relu', input_shape=(input_dim,)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history = model.fit(
    X_train, y_train,
    epochs=50, batch_size=32,
    validation_split=0.1, verbose=1
)

loss, acc = model.evaluate(X_test, y_test)
print(f"[RESULT] Akurasi pada data uji: {acc * 100:.2f}%")

"""### Evaluasi"""

plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Training & Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Training & Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Get predictions from the model
y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)

# Generate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Get the class labels from the LabelEncoder
class_labels = le.classes_

# Plot the confusion matrix
plt.figure(figsize=(15, 12))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.show()

# Generate the classification report as a dictionary
report = classification_report(y_test, y_pred, target_names=le.classes_, output_dict=True)

# Convert to DataFrame dan bulatkan ke 2 angka desimal
report_df = pd.DataFrame(report).transpose().round(2)

# Tampilkan hasil
print("Classification Report:")
print(report_df)

"""### Save Model to H5"""

model.save("gesture_mlp_model.h5")
print("[INFO] Model disimpan sebagai gesture_mlp_model.h5")